[2022-12-29 19:04:31][base_learner.py:338][INFO] [RANK0]: DI-engine DRL Policy
SheepModel(
  (item_encoder): Transformer(
    (embedding): Sequential(
      (0): Linear(in_features=50, out_features=64, bias=True)
      (1): ReLU()
    )
    (act): ReLU()
    (dropout): Dropout(p=0.0, inplace=False)
    (main): Sequential(
      (0): TransformerLayer(
        (attention): Attention(
          (dropout): Dropout(p=0.0, inplace=False)
          (attention_pre): Sequential(
            (0): Linear(in_features=64, out_features=768, bias=True)
          )
          (project): Sequential(
            (0): Linear(in_features=256, out_features=64, bias=True)
          )
        )
        (layernorm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.0, inplace=False)
        (mlp): Sequential(
          (0): Sequential(
            (0): Linear(in_features=64, out_features=128, bias=True)
            (1): ReLU()
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Sequential(
            (0): Linear(in_features=128, out_features=64, bias=True)
            (1): ReLU()
          )
          (3): Dropout(p=0.0, inplace=False)
        )
        (layernorm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerLayer(
        (attention): Attention(
          (dropout): Dropout(p=0.0, inplace=False)
          (attention_pre): Sequential(
            (0): Linear(in_features=64, out_features=768, bias=True)
          )
          (project): Sequential(
            (0): Linear(in_features=256, out_features=64, bias=True)
          )
        )
        (layernorm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.0, inplace=False)
        (mlp): Sequential(
          (0): Sequential(
            (0): Linear(in_features=64, out_features=128, bias=True)
            (1): ReLU()
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Sequential(
            (0): Linear(in_features=128, out_features=64, bias=True)
            (1): ReLU()
          )
          (3): Dropout(p=0.0, inplace=False)
        )
        (layernorm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerLayer(
        (attention): Attention(
          (dropout): Dropout(p=0.0, inplace=False)
          (attention_pre): Sequential(
            (0): Linear(in_features=64, out_features=768, bias=True)
          )
          (project): Sequential(
            (0): Linear(in_features=256, out_features=64, bias=True)
          )
        )
        (layernorm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.0, inplace=False)
        (mlp): Sequential(
          (0): Sequential(
            (0): Linear(in_features=64, out_features=128, bias=True)
            (1): ReLU()
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Sequential(
            (0): Linear(in_features=128, out_features=64, bias=True)
            (1): ReLU()
          )
          (3): Dropout(p=0.0, inplace=False)
        )
        (layernorm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (bucket_encoder): Sequential(
    (0): Linear(in_features=30, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=64, bias=True)
    (5): ReLU()
  )
  (global_encoder): Sequential(
    (0): Linear(in_features=13, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
    (3): ReLU()
  )
  (value_head): Sequential(
    (0): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
      (3): ReLU()
    )
    (1): Linear(in_features=64, out_features=1, bias=True)
  )
)
